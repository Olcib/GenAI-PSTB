{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "19a4d0e8",
   "metadata": {},
   "source": [
    "Daily Challenge: Data Handling and Analysis in Python\n",
    "\n",
    "What You Will Learn\n",
    "Advanced techniques for data normalization, reduction, and aggregation.\n",
    "Skills in gathering, exploring, integrating, and cleaning data using Python.\n",
    "Proficiency in using Pandas for complex data manipulation.\n",
    "\n",
    "Your Task\n",
    "Download and import the Data Science Job Salary dataset.\n",
    "Normalize the ‘salary’ column using Min-Max normalization which scales all salary values between 0 and 1.\n",
    "Implement dimensionality reduction like Principal Component Analysis (PCA) or t-SNE to reduce the number of features (columns) in the dataset.\n",
    "Group the dataset by the ‘experience_level’ column and calculate the average and median salary for each experience level (e.g., Junior, Mid-level, Senior).\n",
    "\n",
    "Hint :\n",
    "As a reminder, normalization is crucial when dealing with data that has different ranges. For example, salary data might have a wide range (e.g., from $20,000 to $200,000). By scaling the data using Min-Max normalization, you make sure that all salary values fall within a consistent range (0 to 1). This is particularly helpful when the data is going to be used in machine learning models, as some algorithms (like k-nearest neighbors or neural networks) perform better when features are normalized. It ensures that no single salary dominates the learning process, making the analysis more balanced.\n",
    "\n",
    "Dimensionality reduction helps simplify complex datasets by reducing the number of variables under consideration. This can make the data more manageable and help avoid the curse of dimensionality—a phenomenon where machine learning models struggle when dealing with high-dimensional data.\n",
    "PCA, for instance, helps in retaining the most important information (variance) from the dataset while reducing noise and redundancy.\n",
    "It can also speed up the training process for models and help in visualizing data in fewer dimensions.\n",
    "\n",
    "Aggregating data helps in understanding trends within subgroups of the dataset.\n",
    "Calculating average and median salaries for each experience level gives insights into the compensation distribution and disparities across different job levels. This kind of aggregation can help in answering business questions like “How does salary evolve with experience?” or “What is the salary distribution for senior-level roles?”"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0d18338a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Your Task\n",
    "#Download and import the Data Science Job Salary dataset. Done.\n",
    "#Normalize the ‘salary’ column using Min-Max normalization which scales all salary values between 0 and 1.\n",
    "#Implement dimensionality reduction like Principal Component Analysis (PCA) or t-SNE to reduce the number of features (columns) in the dataset.\n",
    "#Group the dataset by the ‘experience_level’ column and calculate the average and median salary for each experience level (e.g., Junior, Mid-level, Senior)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a71e1674",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize the ‘salary’ column using Min-Max normalization which scales all salary values between 0 and 1.\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "93f49024",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"C:/Users/cibei/Desktop/PSTB_GenAI/WEEK3/DAY3/DailyChallenge/datascience_salaries.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bcb8d519",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Scaler Initialization :\n",
    "scaler = MinMaxScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f0c0da11",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['Unnamed: 0', 'job_title', 'job_type', 'experience_level', 'location',\n",
      "       'salary_currency', 'salary'],\n",
      "      dtype='object')\n"
     ]
    }
   ],
   "source": [
    "# Column names check up :\n",
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5410a48",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['salary_normalized'] = scaler.fit_transform(df[['salary']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "904de50a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   salary  salary_normalized\n",
      "0  149000           0.601010\n",
      "1  120000           0.454545\n",
      "2   68000           0.191919\n",
      "3  120000           0.454545\n",
      "4  149000           0.601010\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Vérification\n",
    "print(df[['salary', 'salary_normalized']].head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "731bf9ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Implement dimensionality reduction like Principal Component Analysis (PCA) or t-SNE to reduce the number of features (columns) in the dataset.\n",
    "\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "df05a941",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keep only the relevant numerical columns :\n",
    "numeric_df = df.select_dtypes(include='number')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97ead2f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Normalize all numerical columns before reduction:\n",
    "numeric_scaled = scaler.fit_transform(numeric_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0cd38b56",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Group the dataset by the ‘experience_level’ column and calculate the average and median salary for each experience level (e.g., Junior, Mid-level, Senior).datascience_salaries\n",
    "salary_stats = df.groupby('experience_level')['salary'].agg(['mean', 'median']).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7d0cde83",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Rename columns for more clarity\n",
    "salary_stats.columns = ['experience_level', 'average_salary_usd', 'median_salary_usd']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "cf1d56e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  experience_level  average_salary_usd  median_salary_usd\n",
      "0            Entry        36111.111111            30000.0\n",
      "1        Executive        76076.923077            46000.0\n",
      "2              Mid        51786.885246            51000.0\n",
      "3           Senior        75088.033012            68000.0\n"
     ]
    }
   ],
   "source": [
    "print(salary_stats)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
